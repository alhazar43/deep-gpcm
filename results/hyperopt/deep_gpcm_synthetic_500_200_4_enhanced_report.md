# Hyperparameter Optimization Report

**Model:** deep_gpcm  
**Dataset:** synthetic_500_200_4  
**Generated:** 2025-08-14 12:30:15  

## Summary

- **Total Trials:** 47
- **Successful Trials:** 47 (100.0%)
- **Best Score:** 0.6233
- **Mean Score:** 0.6144 ¬± 0.0080
- **Total Time:** 8435.0s
- **Convergence Trial:** 31

## Best Parameters

- **memory_size:** 20
- **final_fc_dim:** 100
- **dropout_rate:** 0.049646
- **ce_weight_logit:** -0.252597
- **focal_weight_logit:** 0.918033

## Parameter Importance

- **memory_size:** 86.2%
- **focal_weight_logit:** 7.1%
- **ce_weight_logit:** 3.9%
- **dropout_rate:** 2.1%
- **final_fc_dim:** 0.7%

## Parameter-Performance Correlations

- **memory_size:** -0.877
- **ce_weight_logit:** -0.180
- **focal_weight_logit:** 0.135
- **final_fc_dim:** 0.078
- **dropout_rate:** -0.030

## Convergence Analysis

- **Convergence Trial:** 5
- **Final Best Score:** 0.6233
- **Improvement Rate:** 0.0005
- **Convergence Efficiency:** 10.6%

## Optimization Efficiency

- **Score Range:** 0.0278
- **Exploration Ratio:** 0.287
- **Improvement Rate:** 0.0041
- **95th Percentile Score:** 0.6230

## ü§ñ Automated Analysis & Recommendations

*This section provides AI-generated insights and actionable recommendations based on the optimization results.*

### üìä Performance Summary

- Best performance: 0.6233 QWK (Trial #31)
- Performance range: 0.0278 spread across all trials
- Convergence efficiency: Excellent (10.6% of trials needed)

### üîç Parameter Patterns

- memory_size is critical (86.2% importance) - focus optimization here
- Top 3 parameters dominate (97.2%): memory_size, focal_weight_logit, ce_weight_logit
- Lower memory_size values strongly improve performance (correlation: -0.877)

### ‚öñÔ∏è Loss Function Insights

- Optimal loss combination: CE=0.18, Focal=0.58, QWK=0.23
- Focal loss dominance suggests class imbalance handling is critical

### üöÄ Actionable Recommendations

- Focus future optimization on memory_size (86.2% importance)
- Efficient convergence - current search space and strategy are effective
- Small memory networks work best - consider testing even smaller sizes (10-15)
- Very low dropout works best - try 0.01-0.03 range for fine-tuning
- Expand search space: add embed_dim, key_dim, n_heads, value_dim for architectural optimization

### üìã Next Steps

- Good performance - fine-tune with extended search space and adaptive epochs
- Implement adaptive epoch allocation (5‚Üí20‚Üí40 epochs based on performance)
- Add learning rate scheduling and optimizer parameters to search space

## Visualizations

- **Main Analysis:** `results/hyperopt/deep_gpcm_synthetic_500_200_4_hyperopt_analysis.png`
- **Detailed Parameters:** `results/hyperopt/deep_gpcm_synthetic_500_200_4_detailed_parameters.png`

---
*Generated by Deep-GPCM Hyperparameter Optimization System*