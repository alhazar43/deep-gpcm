# Hyperparameter Optimization Report

**Model:** attn_gpcm_learn  
**Dataset:** synthetic_500_200_4  
**Generated:** 2025-08-14 20:40:32  

## Summary

- **Total Trials:** 50
- **Successful Trials:** 50 (100.0%)
- **Best Score:** 0.6370
- **Mean Score:** 0.5561 ¬± 0.1372
- **Total Time:** 8446.9s
- **Convergence Trial:** 48

## Best Parameters

- **memory_size:** 50
- **final_fc_dim:** 50
- **dropout_rate:** 0.200000
- **embed_dim:** 32
- **n_heads:** 4
- **n_cycles:** 2
- **ce_weight_logit:** 1.679634
- **focal_weight_logit:** -0.697832
- **key_dim:** 128
- **value_dim:** 256
- **lr:** 0.005895
- **weight_decay:** 0.000307
- **batch_size:** 32
- **grad_clip:** 5.000000
- **label_smoothing:** 0.200000

## Parameter Importance

- **label_smoothing:** 50.7%
- **dropout_rate:** 13.4%
- **ce_weight_logit:** 8.3%
- **lr:** 8.3%
- **batch_size:** 4.8%
- **grad_clip:** 3.9%
- **embed_dim:** 3.9%
- **key_dim:** 2.5%
- **focal_weight_logit:** 1.4%
- **final_fc_dim:** 1.3%
- **n_heads:** 0.7%
- **memory_size:** 0.3%
- **weight_decay:** 0.2%
- **n_cycles:** 0.1%
- **value_dim:** 0.0%

## Parameter-Performance Correlations

- **label_smoothing:** 0.872
- **key_dim:** 0.856
- **ce_weight_logit:** 0.818
- **grad_clip:** 0.785
- **batch_size:** -0.776
- **final_fc_dim:** -0.735
- **dropout_rate:** 0.651
- **weight_decay:** -0.615
- **embed_dim:** -0.608
- **lr:** -0.449
- **value_dim:** -0.278
- **memory_size:** -0.251
- **focal_weight_logit:** 0.154
- **n_cycles:** -0.025
- **n_heads:** 0.015

## Convergence Analysis

- **Convergence Trial:** 5
- **Final Best Score:** 0.6370
- **Improvement Rate:** 0.0093
- **Convergence Efficiency:** 10.0%

## Optimization Efficiency

- **Score Range:** 0.5090
- **Exploration Ratio:** 0.267
- **Improvement Rate:** 0.0455
- **95th Percentile Score:** 0.6318

## ü§ñ Automated Analysis & Recommendations

*This section provides AI-generated insights and actionable recommendations based on the optimization results.*

### üìä Performance Summary

- Best performance: 0.6370 QWK (Trial #48)
- Performance range: 0.5090 spread across all trials
- Convergence efficiency: Excellent (10.0% of trials needed)

### üîç Parameter Patterns

- label_smoothing is critical (50.7% importance) - focus optimization here
- Lower final_fc_dim values strongly improve performance (correlation: -0.735)
- Lower embed_dim values strongly improve performance (correlation: -0.608)
- Lower weight_decay values strongly improve performance (correlation: -0.615)
- Lower batch_size values strongly improve performance (correlation: -0.776)
- Higher dropout_rate values strongly improve performance (correlation: 0.651)
- Higher ce_weight_logit values strongly improve performance (correlation: 0.818)
- Higher key_dim values strongly improve performance (correlation: 0.856)
- Higher grad_clip values strongly improve performance (correlation: 0.785)
- Higher label_smoothing values strongly improve performance (correlation: 0.872)

### ‚öñÔ∏è Loss Function Insights

- Optimal loss combination: CE=0.78, Focal=0.07, QWK=0.15

### üöÄ Actionable Recommendations

- Focus future optimization on label_smoothing (50.7% importance)
- Efficient convergence - current search space and strategy are effective
- Higher regularization needed - current dropout level is appropriate

### üìã Next Steps

- Performance below 65% QWK - expand architectural parameters and increase trial budget
- Implement adaptive epoch allocation (5‚Üí20‚Üí40 epochs based on performance)
- Add learning rate scheduling and optimizer parameters to search space

## Visualizations

- **Main Analysis:** `results/hyperopt/attn_gpcm_learn_synthetic_500_200_4_hyperopt_analysis.png`
- **Detailed Parameters:** `results/hyperopt/attn_gpcm_learn_synthetic_500_200_4_detailed_parameters.png`

---
*Generated by Deep-GPCM Hyperparameter Optimization System*
