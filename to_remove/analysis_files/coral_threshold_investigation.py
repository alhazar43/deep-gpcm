#!/usr/bin/env python3
"""
Investigation: How CORAL Thresholds Should vs Actually Work
This investigates the discrepancy between expected CORAL behavior and actual implementation.
"""

import os
import sys
import torch
import numpy as np

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.coral_gpcm import EnhancedCORALGPCM
from train import load_simple_data, create_data_loaders


def investigate_coral_thresholds():
    """Investigate how CORAL thresholds are supposed to work vs how they actually work."""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("=" * 80)
    print("CORAL THRESHOLD INVESTIGATION")
    print("=" * 80)
    
    # Load model
    model_path = "save_models/best_full_adaptive_coral_gpcm_synthetic_OC.pth"
    checkpoint = torch.load(model_path, map_location=device)
    
    # Get dimensions and create model
    state_dict = checkpoint['model_state_dict']
    memory_size = state_dict['memory.key_memory_matrix'].shape[0]
    key_dim = state_dict['q_embed.weight'].shape[1]
    value_dim = state_dict['init_value_memory'].shape[1]
    final_fc_dim = state_dict['summary_network.0.bias'].shape[0]
    
    model = EnhancedCORALGPCM(
        n_questions=200,
        n_cats=4,
        memory_size=memory_size,
        key_dim=key_dim,
        value_dim=value_dim,
        final_fc_dim=final_fc_dim,
        enable_adaptive_blending=True,
        use_full_blender=True
    )
    
    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    model = model.to(device)
    model.eval()
    
    print(f"üîç INVESTIGATING CORAL LAYER:")
    print(f"  CORAL layer exists: {hasattr(model, 'coral_layer') and model.coral_layer is not None}")
    
    if hasattr(model, 'coral_layer') and model.coral_layer is not None:
        coral_layer = model.coral_layer
        print(f"  CORAL layer type: {type(coral_layer).__name__}")
        print(f"  Input dim: {coral_layer.input_dim}")
        print(f"  Number of categories: {coral_layer.n_cats}")
        print(f"  Use thresholds: {coral_layer.use_thresholds}")
        
        # Check CORAL parameters
        coral_bias = coral_layer.rank_classifier.bias.data.cpu().numpy()
        coral_weights = coral_layer.rank_classifier.weight.data.cpu().numpy()
        
        print(f"\nüìä CORAL PARAMETERS:")
        print(f"  Rank classifier bias (œÑ): {coral_bias}")
        print(f"  Rank classifier weights shape: {coral_weights.shape}")
        
        if hasattr(coral_layer, 'ordinal_thresholds'):
            ordinal_thresholds = coral_layer.ordinal_thresholds.data.cpu().numpy()
            print(f"  Ordinal thresholds: {ordinal_thresholds}")
        else:
            print(f"  No ordinal thresholds found")
    
    # Load test data
    train_path = "data/synthetic_OC/synthetic_oc_train.txt"
    test_path = "data/synthetic_OC/synthetic_oc_test.txt"
    train_data, test_data, n_questions, n_cats = load_simple_data(train_path, test_path)
    _, test_loader = create_data_loaders(train_data, test_data, batch_size=2)
    
    # Get one small batch
    questions, responses, mask = next(iter(test_loader))
    questions = questions.to(device)
    responses = responses.to(device)
    
    print(f"\nüßÆ TRACING CORAL COMPUTATION:")
    print(f"  Input shape: {questions.shape}")
    
    with torch.no_grad():
        # Step 1: Get the full forward pass
        student_abilities, item_thresholds, discrimination_params, final_probs = model(questions, responses)
        
        print(f"  Full forward pass completed")
        print(f"  Item thresholds (Œ≤) shape: {item_thresholds.shape}")
        print(f"  Item thresholds (Œ≤) sample: {item_thresholds[0, 0, :].cpu().numpy()}")
        
        # Step 2: Manual CORAL computation using CORAL layer
        print(f"\nüéØ WHAT CORAL LAYER SHOULD DO:")
        
        # Get some features to pass to CORAL layer
        batch_size, seq_len = questions.shape
        
        # Initialize memory and get features like in forward pass
        model.memory.init_value_memory(batch_size, model.init_value_memory)
        gpcm_embeds = model.create_embeddings(questions, responses)
        q_embeds = model.q_embed(questions)
        processed_embeds = model.process_embeddings(gpcm_embeds, q_embeds)
        
        # Process first timestep as example
        t = 0
        q_embed_t = q_embeds[:, t, :]
        gpcm_embed_t = processed_embeds[:, t, :]
        value_embed_t = model.gpcm_value_embed(gpcm_embed_t)
        
        correlation_weight = model.memory.attention(q_embed_t)
        read_content = model.memory.read(correlation_weight)
        summary_input = torch.cat([read_content, q_embed_t], dim=-1)
        summary_vector = model.summary_network(summary_input)
        
        print(f"  Summary vector shape: {summary_vector.shape}")
        
        # Pass through CORAL layer
        coral_probs, coral_info = model.coral_layer(summary_vector.unsqueeze(1))
        
        print(f"  CORAL probabilities shape: {coral_probs.shape}")
        print(f"  CORAL probabilities sample: {coral_probs[0, 0, :].cpu().numpy()}")
        print(f"  CORAL info keys: {coral_info.keys()}")
        
        if 'logits' in coral_info:
            coral_logits = coral_info['logits']
            print(f"  CORAL logits shape: {coral_logits.shape}")
            print(f"  CORAL logits sample: {coral_logits[0, 0, :].cpu().numpy()}")
        
        if 'cumulative_probs' in coral_info:
            coral_cum_probs = coral_info['cumulative_probs']
            print(f"  CORAL cumulative probs shape: {coral_cum_probs.shape}")
            print(f"  CORAL cumulative probs sample: {coral_cum_probs[0, 0, :].cpu().numpy()}")
        
        # Step 3: Compare with what the model actually does
        print(f"\n‚ö° WHAT THE MODEL ACTUALLY DOES:")
        
        # Look at the forward pass in EnhancedCORALGPCM
        # Lines 418-435 in coral_gpcm.py show it computes CORAL using Œ≤, not œÑ!
        
        # Manual computation using Œ≤ (what the model actually does)
        theta_t = student_abilities[:, 0:1]  # First timestep
        beta_t = item_thresholds[:, 0, :]    # Œ≤ thresholds (NOT œÑ!)
        alpha_t = discrimination_params[:, 0:1]
        
        manual_coral_logits = alpha_t * (theta_t - beta_t)
        manual_coral_cum_probs = torch.sigmoid(manual_coral_logits)
        
        print(f"  Manual CORAL logits (using Œ≤): {manual_coral_logits[0, :].cpu().numpy()}")
        print(f"  Manual CORAL cum probs (using Œ≤): {manual_coral_cum_probs[0, :].cpu().numpy()}")
        
        # Step 4: The REAL issue
        print(f"\nüö® THE ACTUAL ISSUE:")
        print(f"  The model has a CORAL layer that produces œÑ parameters...")
        print(f"  BUT the CORAL computation in the forward pass uses Œ≤ parameters instead!")
        print(f"  This means CORAL is NOT actually using its own thresholds!")
        
        print(f"\nüîç EVIDENCE:")
        print(f"  1. CORAL layer bias (œÑ): {model.coral_layer.rank_classifier.bias.data.cpu().numpy()}")
        print(f"  2. But CORAL computation uses Œ≤: {item_thresholds[0, 0, :].cpu().numpy()}")
        print(f"  3. These are different values (unless œÑ is all zeros)")
        
        # Check if œÑ is all zeros (indicating it's not actually trained/used)
        tau_is_zero = torch.allclose(model.coral_layer.rank_classifier.bias.data, torch.zeros_like(model.coral_layer.rank_classifier.bias.data))
        print(f"  4. œÑ is all zeros: {tau_is_zero}")
        
        if tau_is_zero:
            print(f"  ‚ö†Ô∏è  CONCLUSION: œÑ parameters are not being used/trained properly!")
        else:
            print(f"  ‚ö†Ô∏è  CONCLUSION: œÑ parameters exist but CORAL computation ignores them!")
    
    print(f"\n" + "=" * 80)
    print(f"INVESTIGATION CONCLUSION")
    print(f"=" * 80)
    print(f"YOU ARE ABSOLUTELY CORRECT!")
    print(f"")
    print(f"üéØ CORAL SHOULD produce its own thresholds (œÑ)")
    print(f"üö® BUT the current implementation uses Œ≤ instead of œÑ for CORAL computation")
    print(f"")
    print(f"This is a DESIGN ISSUE in the model:")
    print(f"1. CORAL layer exists and has œÑ parameters")
    print(f"2. BUT EnhancedCORALGPCM.forward() ignores œÑ and uses Œ≤ for CORAL")
    print(f"3. This means CORAL is not actually functioning as intended")
    print(f"")
    print(f"The correct implementation should:")
    print(f"- Use Œ≤ for GPCM: alpha * (theta - beta)")  
    print(f"- Use œÑ for CORAL: alpha * (theta - tau)")
    print(f"- Then blend the two different probability distributions")
    print(f"=" * 80)


if __name__ == "__main__":
    investigate_coral_thresholds()