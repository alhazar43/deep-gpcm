# Deep-GPCM Mathematical Foundations

## Overview

This document provides the complete mathematical formulation for the Deep-GPCM system, including DKVMN memory networks, IRT parameter extraction, GPCM probability computation, CORAL ordinal regression, and adaptive blending mechanisms.

## Core Architecture Mathematical Flow

```
Input ‚Üí Embeddings ‚Üí DKVMN ‚Üí IRT Parameters ‚Üí GPCM/CORAL ‚Üí Adaptive Blending ‚Üí Output
```

## 1. DKVMN Memory Network Mathematics

### 1.1 Memory Architecture

**Memory Matrices:**
- **Key Memory**: `M_k ‚àà ‚Ñù^(N√ód_k)` - Static concept representations
- **Value Memory**: `M_v ‚àà ‚Ñù^(N√ód_v)` - Dynamic mastery states

Where:
- `N`: Memory size (number of concepts)
- `d_k`: Key dimension 
- `d_v`: Value dimension

### 1.2 Attention Mechanism

**Attention Weights:**
```
w_t^(i) = softmax(q_t^T M_k^(i))
```

**Read Operation:**
```
r_t = Œ£_{i=1}^N w_t^(i) M_v^(i)
```

Where:
- `q_t ‚àà ‚Ñù^{d_k}`: Question embedding at time t
- `r_t ‚àà ‚Ñù^{d_v}`: Read content vector

### 1.3 Memory Update

**Erase Operation:**
```
M_v^(i)_{new} = M_v^(i)_{old} ‚äô (1 - w_t^(i) e_t^T)
```

**Add Operation:**
```
M_v^(i)_{final} = M_v^(i)_{new} + w_t^(i) a_t^T
```

Where:
- `e_t ‚àà ‚Ñù^{d_v}`: Erase vector (what to forget)
- `a_t ‚àà ‚Ñù^{d_v}`: Add vector (what to remember)
- `‚äô`: Element-wise multiplication

## 2. IRT Parameter Extraction

### 2.1 Summary Vector Creation

**Memory-Question Integration:**
```
s_t = tanh(W_s [r_t; q_t] + b_s)
```

Where:
- `s_t ‚àà ‚Ñù^{d_s}`: Summary vector
- `W_s ‚àà ‚Ñù^{d_s√ó(d_v+d_k)}`: Summary weight matrix
- `[r_t; q_t]`: Concatenation of read content and question embedding

### 2.2 IRT Parameter Computation

**Student Ability (Œ∏):**
```
Œ∏_t = tanh(W_Œ∏ s_t + b_Œ∏)
```

**Item Discrimination (Œ±):**
```
Œ±_t = softplus(W_Œ± [s_t; q_t] + b_Œ±) + Œµ_Œ±
```

**Item Thresholds (Œ≤):**
```
Œ≤_{t,k} = Œ£_{j=0}^k softplus(W_Œ≤^{(j)} [s_t; q_t] + b_Œ≤^{(j)})
```

Where:
- `Œ∏_t ‚àà ‚Ñù`: Student ability at time t
- `Œ±_t ‚àà ‚Ñù`: Item discrimination at time t
- `Œ≤_{t,k} ‚àà ‚Ñù`: Cumulative threshold for category k
- `Œµ_Œ± = 0.1`: Minimum discrimination to ensure positivity
- `softplus(x) = log(1 + exp(x))`: Ensures monotonic thresholds

## 3. GPCM Probability Computation

### 3.1 Generalized Partial Credit Model

**Cumulative Logits:**
```
Z_{t,k} = Œ£_{j=0}^k Œ±_t(Œ∏_t - Œ≤_{t,j})
```

**Category Probabilities:**
```
P(Y_t = k | Œ∏_t, Œ±_t, Œ≤_t) = exp(Z_{t,k}) / Œ£_{c=0}^{K-1} exp(Z_{t,c})
```

Where:
- `Y_t`: Response at time t
- `K`: Number of categories (4 for Deep-GPCM)
- `Z_{t,0} = 0` by convention

### 3.2 Mathematical Properties

**Monotonicity Constraint:**
```
Œ≤_{t,0} < Œ≤_{t,1} < Œ≤_{t,2} < ... < Œ≤_{t,K-2}
```

**Probability Normalization:**
```
Œ£_{k=0}^{K-1} P(Y_t = k | Œ∏_t, Œ±_t, Œ≤_t) = 1
```

## 4. CORAL Ordinal Regression

### 4.1 CORAL Framework

**Ordinal Thresholds (œÑ):**
```
œÑ = [œÑ_0, œÑ_1, œÑ_2, ..., œÑ_{K-2}]
```

**Cumulative Logits:**
```
f_k = h(x) - œÑ_k
```

Where:
- `h(x)`: Neural network output (scalar)
- `œÑ_k`: Learnable ordinal threshold

### 4.2 CORAL Probability Computation

**Binary Probabilities:**
```
P(Y > k) = œÉ(f_k) = œÉ(h(x) - œÑ_k)
```

**Category Probabilities:**
```
P(Y = 0) = 1 - P(Y > 0)
P(Y = k) = P(Y > k-1) - P(Y > k)  for k ‚àà {1, ..., K-2}
P(Y = K-1) = P(Y > K-2)
```

Where `œÉ(x) = 1/(1 + exp(-x))` is the sigmoid function.

### 4.3 CRITICAL ISSUE: Current Implementation Flaw

**Intended CORAL Computation:**
```
P_CORAL(Y_t = k) = CORAL_structure(Œ±_t(Œ∏_t - œÑ_k))
```

**Actual (Incorrect) Implementation:**
```
P_CORAL(Y_t = k) = CORAL_structure(Œ±_t(Œ∏_t - Œ≤_{t,k}))
```

**Problem**: CORAL uses GPCM Œ≤ parameters instead of dedicated œÑ parameters, making both systems identical.

## 5. Adaptive Blending Mathematics

### 5.1 Threshold Distance Analysis

**Semantic Threshold Alignment:**
```
d_{i,j} = |œÑ_i - Œ≤_{j,i}|  ‚àÄi ‚àà {0, 1, 2}
```

Where:
- `œÑ_i`: CORAL threshold for boundary i
- `Œ≤_{j,i}`: GPCM threshold for item j, boundary i

**Distance Statistics:**
```
d_min = min_i(d_{i,j})
d_avg = (1/K) Œ£_i d_{i,j}
d_spread = std(d_{0,j}, d_{1,j}, d_{2,j})
```

### 5.2 Geometric Analysis

**Range Divergence:**
```
R_GPCM = max_i(Œ≤_{j,i}) - min_i(Œ≤_{j,i})
R_CORAL = max_i(œÑ_i) - min_i(œÑ_i)
ŒîR = |R_CORAL - R_GPCM| / (R_CORAL + R_GPCM + Œµ)
```

**Threshold Correlation:**
```
œÅ = (Œ£_i Œ≤_{j,i} œÑ_i) / (||Œ≤_j|| ||œÑ|| + Œµ)
```

### 5.3 BGT (Bounded Geometric Transform) Framework

**Problem**: Original operations cause gradient explosion:
```
log(1 + x) ‚Üí ‚àû  as x ‚Üí ‚àû
x/(1 + x) ‚Üí singularities near x = -1
```

**BGT Solutions:**
```
log_BGT(x) = 2 tanh(clamp(x, 0, 10) / 2)        ‚àà [0, 2]
div_BGT(x) = œÉ(clamp(x, -10, 10))               ‚àà [0, 1]
corr_BGT(x) = 0.5(1 + tanh(clamp(x, -3, 3)))   ‚àà [0, 1]
```

### 5.4 Adaptive Weight Computation

**BGT-Stabilized Weight Formula:**
```
w_{j,k} = œÉ(clamp(
    Œª_R ¬∑ log_BGT(ŒîR_j) + 
    Œª_D ¬∑ div_BGT(d_min,j) + 
    Œª_C ¬∑ corr_BGT(œÅ_j) + 
    Œª_S ¬∑ exp(-clamp(d_spread,j, 0, 5)) + 
    b_0,
    -3, 3
))
```

Where:
- `Œª_R ‚àà [0.1, 2.0]`: Range sensitivity (learnable)
- `Œª_D ‚àà [0.5, 3.0]`: Distance sensitivity (learnable)
- `Œª_C = 0.3`: Correlation weight (fixed)
- `Œª_S = 0.1`: Spread penalty weight (fixed)
- `b_0 ‚àà [-1.0, 1.0]`: Baseline bias (learnable)

### 5.5 Final Probability Blending

**Category-Specific Blending:**
```
P_final(Y_t = k) = (1 - w_{t,k}) P_GPCM(Y_t = k) + w_{t,k} P_CORAL(Y_t = k)
```

**Normalization:**
```
P_final(Y_t = k) ‚Üê P_final(Y_t = k) / Œ£_{c=0}^{K-1} P_final(Y_t = c)
```

## 6. Loss Functions

### 6.1 Cross-Entropy Loss

**Standard Classification Loss:**
```
‚Ñí_CE = -Œ£_t Œ£_k y_{t,k} log(P_final(Y_t = k))
```

### 6.2 Quadratic Weighted Kappa Loss

**QWK Differentiable Approximation:**
```
W_{i,j} = (i - j)¬≤ / (K - 1)¬≤
```

**Soft Confusion Matrix:**
```
C_{i,j} = Œ£_t Œ¥(y_t = i) P_final(Y_t = j)
```

**QWK Computation:**
```
p_o = Œ£_{i,j} C_{i,j}(1 - W_{i,j}) / Œ£_{i,j} C_{i,j}
p_e = Œ£_{i,j} (Œ£_k C_{i,k})(Œ£_k C_{k,j})(1 - W_{i,j}) / (Œ£_{i,j} C_{i,j})¬≤
QWK = (p_o - p_e) / (1 - p_e + Œµ)
```

**QWK Loss:**
```
‚Ñí_QWK = -QWK
```

### 6.3 CORAL Ordinal Loss

**Cumulative Link Loss:**
```
‚Ñí_CORAL = -Œ£_t Œ£_{k=0}^{K-2} [y_t > k] log(œÉ(f_{t,k})) + [y_t ‚â§ k] log(1 - œÉ(f_{t,k}))
```

### 6.4 Combined Loss

**Weighted Combination:**
```
‚Ñí_total = Œª_CE ‚Ñí_CE + Œª_QWK ‚Ñí_QWK + Œª_CORAL ‚Ñí_CORAL
```

With typical weights: `Œª_CE = 0.5`, `Œª_QWK = 0.3`, `Œª_CORAL = 0.5`

## 7. Gradient Flow Analysis

### 7.1 Critical Gradient Paths

**Memory Updates:**
```
‚àÇ‚Ñí/‚àÇM_v^(i) = Œ£_t w_t^(i) ‚àÇ‚Ñí/‚àÇr_t
```

**IRT Parameters:**
```
‚àÇ‚Ñí/‚àÇŒ∏_t = Œ±_t Œ£_k (P_final(Y_t = k) - y_{t,k}) g_k
‚àÇ‚Ñí/‚àÇŒ±_t = Œ£_k (P_final(Y_t = k) - y_{t,k}) (Œ∏_t - Œ≤_{t,k}) g_k
‚àÇ‚Ñí/‚àÇŒ≤_{t,k} = -Œ±_t Œ£_{j‚â•k} (P_final(Y_t = j) - y_{t,j}) g_j
```

Where `g_k` represents the GPCM gradient contribution for category k.

### 7.2 Gradient Isolation for Adaptive Blending

**Problem**: Gradient coupling between adaptive weights and memory updates:
```
‚àÇ‚Ñí/‚àÇw_{t,k} ‚àù ‚àÇd_min/‚àÇŒ≤_{t,k} ‚Üí ‚àÇ‚Ñí/‚àÇM_v
```

**Solution**: Strategic gradient detachment:
```
w_{t,k} = f(Œ≤_{t,k}.detach(), œÑ_k, Œ∏_t, Œ±_t)
```

This breaks the gradient flow while preserving adaptive behavior.

## 8. Numerical Stability Considerations

### 8.1 Stability Constraints

**Parameter Bounds:**
```
Œ∏_t ‚àà [-3, 3]      (tanh activation)
Œ±_t ‚àà [0.1, 3.0]   (softplus + Œµ)
Œ≤_{t,k} ‚àà ‚Ñù       (monotonic via cumsum)
œÑ_k ‚àà ‚Ñù           (learnable thresholds)
```

**Probability Clamping:**
```
P(Y_t = k) ‚àà [Œµ, 1-Œµ]  where Œµ = 1e-7
```

### 8.2 BGT Stability Guarantees

**Bounded Outputs:**
```
log_BGT(x) ‚àà [0, 2]
div_BGT(x) ‚àà [0, 1]  
corr_BGT(x) ‚àà [0, 1]
```

**Bounded Gradients:**
```
|‚àá log_BGT(x)| ‚â§ 1
|‚àá div_BGT(x)| ‚â§ 0.25
|‚àá corr_BGT(x)| ‚â§ 0.5
```

## 9. Critical Mathematical Issues

### 9.1 ‚ùå CORAL Design Flaw (BLOCKING)

**Problem**: Current CORAL implementation uses Œ≤ parameters instead of œÑ thresholds, making CORAL and GPCM computations mathematically identical.

**Current (INCORRECT) Implementation**:
```
P_CORAL(Y_t = k) = œÉ(Œ±_t(Œ∏_t - Œ≤_{t,k}))  // Uses Œ≤ parameters
P_GPCM(Y_t = k) = œÉ(Œ±_t(Œ∏_t - Œ≤_{t,k}))   // Also uses Œ≤ parameters
```

**Correct Mathematical Formulation**:
```
P_CORAL(Y_t = k) = œÉ(Œ±_t(Œ∏_t - œÑ_k))      // Should use œÑ thresholds  
P_GPCM(Y_t = k) = œÉ(Œ±_t(Œ∏_t - Œ≤_{t,k}))   // Correctly uses Œ≤ parameters
```

**Evidence from Parameter Extraction**:
- CORAL œÑ parameters: `[0., 0., 0.]` (all zeros, unused)
- GPCM Œ≤ parameters: `[-0.8234, -0.0156, 0.7453]` (actively learned)
- Both systems use identical Œ≤ values in computation

**Mathematical Impact**:
```
P_CORAL ‚â° P_GPCM  ‚üπ  Adaptive Blending = Identity Transform
w_{t,k} P_CORAL + (1-w_{t,k}) P_GPCM = P_GPCM  (no benefit)
```

**Status**: üìã Fix required before any CORAL-related research is valid.

### 9.2 Validated Performance Metrics

**Current Verified Results** (from `/results/train/`):

| Model | Categorical Acc. | QWK | Ordinal Acc. | Status |
|-------|-----------------|-----|--------------|--------|
| **Deep-GPCM** | **53.5%** (¬±1.3%) | **0.643** (¬±0.016) | **83.1%** (¬±0.7%) | ‚úÖ VALID |
| **Full Adaptive CORAL-GPCM** | **53.7%** (¬±1.1%) | **0.681** (¬±0.012) | **87.4%** (¬±0.4%) | ‚ùå INVALID |

**Critical Note**: CORAL results are invalid due to Œ≤/œÑ parameter confusion.

## 10. Model Complexity Analysis

### 10.1 Parameter Counts

**DeepGPCM:**
- Memory: `N √ó (d_k + d_v)` = 50 √ó (50 + 200) = 12,500
- Embeddings: `(Q + 1) √ó d_k` = 401 √ó 50 = 20,050
- IRT extraction: `~6,000` parameters
- GPCM value transform: `1,600 √ó 200` = 320,000
- **Total**: ~448,555 parameters

**CORAL Extension:**
- CORAL projection: `d_v √ó (K-1)` = 200 √ó 3 = 600
- Threshold parameters: `K-1` = 3
- **Additional**: ~603 parameters

**Adaptive Blending:**
- Range sensitivity: 1
- Distance sensitivity: 1  
- Baseline bias: 1
- **Additional**: 3 parameters

### 9.2 Computational Complexity

**Forward Pass:** `O(N √ó d_v + Q √ó d_k + T √ó K)`
**Memory Update:** `O(T √ó N √ó d_v)`
**IRT Extraction:** `O(T √ó d_s √ó K)`
**Adaptive Blending:** `O(T √ó K¬≤)`

Where:
- `T`: Sequence length
- `N`: Memory size
- `Q`: Number of questions
- `K`: Number of categories

## 10. Mathematical Validation Properties

### 10.1 Probability Axioms

**Non-negativity:**
```
P(Y_t = k) ‚â• 0  ‚àÄk, t
```

**Normalization:**
```
Œ£_{k=0}^{K-1} P(Y_t = k) = 1  ‚àÄt
```

**Monotonicity (CORAL):**
```
P(Y > 0) ‚â• P(Y > 1) ‚â• ... ‚â• P(Y > K-2) ‚â• 0
```

### 10.2 Gradient Properties

**Gradient Bounds:**
```
||‚àá_Œ∏ ‚Ñí|| ‚â§ C_Œ∏
||‚àá_Œ± ‚Ñí|| ‚â§ C_Œ±  
||‚àá_Œ≤ ‚Ñí|| ‚â§ C_Œ≤
```

With BGT framework ensuring `C_* < ‚àû`.

### 10.3 Convergence Properties

**Loss Monotonicity:**
```
‚Ñí(t+1) ‚â§ ‚Ñí(t) + Œ¥
```

Where `Œ¥ ‚Üí 0` as training progresses.

---

## Critical Mathematical Issues

### Issue 1: CORAL Parameter Usage (CRITICAL)

**Current Implementation:**
```
P_CORAL(Y_t = k) = CORAL_structure(Œ±_t(Œ∏_t - Œ≤_{t,k}))  ‚ùå
```

**Correct Implementation:**
```
P_CORAL(Y_t = k) = CORAL_structure(Œ±_t(Œ∏_t - œÑ_k))      ‚úÖ
```

### Issue 2: Temporal vs Static Parameters

**Neural Model (Temporal):**
```
Œ∏_t, Œ±_t, Œ≤_{t,k} = f(memory_state_t, question_t)
```

**Classical IRT (Static):**
```
Œ∏_i, Œ±_j, Œ≤_{j,k} = constants
```

This fundamental difference explains poor parameter recovery correlations.

---

**Mathematical Framework Status**: Complete foundation with identified critical issues requiring architectural fixes.